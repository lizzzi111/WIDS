{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows    = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV6Ca0tct9t6"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#                             #\n",
    "#        ENCODE FACTORS       #\n",
    "#                             #\n",
    "###############################\n",
    "\n",
    "# performs label encoding\n",
    "def reduce_mem_usage(df, verbose = True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "def label_encoding(df_train, df_valid, df_test):\n",
    "    \n",
    "    factors = df_train.select_dtypes('object').columns\n",
    "    \n",
    "    lbl = LabelEncoder()\n",
    "\n",
    "    for f in factors:        \n",
    "        #print(f)\n",
    "        lbl.fit(list(df_train[f].values)+ list(df_test[f].values) + list(df_valid[f].values) )\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_valid[f] = lbl.transform(list(df_valid[f].values))\n",
    "        df_test[f]  = lbl.transform(list(df_test[f].values))\n",
    "\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "from sklearn import base\n",
    "class KFoldTargetEncoderTrain(base.BaseEstimator,\n",
    "                               base.TransformerMixin):\n",
    "    def __init__(self,colnames,targetName,\n",
    "                  n_fold=5, verbosity=True,\n",
    "                  discardOriginal_col=False):\n",
    "        self.colnames = colnames\n",
    "        self.targetName = targetName\n",
    "        self.n_fold = n_fold\n",
    "        self.verbosity = verbosity\n",
    "        self.discardOriginal_col = discardOriginal_col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        assert(type(self.targetName) == str)\n",
    "        assert(type(self.colnames) == str)\n",
    "        assert(self.colnames in X.columns)\n",
    "        assert(self.targetName in X.columns)\n",
    "        mean_of_target = X[self.targetName].mean()\n",
    "        kf = KFold(n_splits = self.n_fold,\n",
    "                   shuffle = False, random_state=2019)\n",
    "        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n",
    "        X[col_mean_name] = np.nan\n",
    "        for tr_ind, val_ind in kf.split(X):\n",
    "            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n",
    "            X.loc[X.index[val_ind], col_mean_name] =  X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())\n",
    "            X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
    "        if self.verbosity:\n",
    "            encoded_feature = X[col_mean_name].values\n",
    "            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,self.targetName, np.corrcoef(X[self.targetName].values, encoded_feature)[0][1]))\n",
    "        if self.discardOriginal_col:\n",
    "            X = X.drop(self.targetName, axis=1)\n",
    "        return X\n",
    "\n",
    "class KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,train,colNames,encodedName):\n",
    "        \n",
    "        self.train = train\n",
    "        self.colNames = colNames\n",
    "        self.encodedName = encodedName\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        mean =  self.train[[self.colNames,\n",
    "                self.encodedName]].groupby(\n",
    "                                self.colNames).mean().reset_index() \n",
    "        \n",
    "        dd = {}\n",
    "        for index, row in mean.iterrows():\n",
    "            dd[row[self.colNames]] = row[self.encodedName]\n",
    "        X[self.encodedName] = X[self.colNames]\n",
    "        X = X.replace({self.encodedName: dd})\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kraLQ_ACs1DU"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from scipy.stats.mstats import winsorize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import log_loss, roc_auc_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnKQLwT_s1Da"
   },
   "outputs": [],
   "source": [
    "############ RANDOMNESS\n",
    "\n",
    "# seed function\n",
    "def seed_everything(seed = 42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# set seed\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTrTKS7gs1Db"
   },
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3686,
     "status": "ok",
     "timestamp": 1579793991092,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64",
      "userId": "01253993997636551956"
     },
     "user_tz": -60
    },
    "id": "Btmfn4ars1Dc",
    "outputId": "d25b01c0-b400-4a96-82d4-eb858eee5c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110369, 186)\n",
      "(39308, 186)\n"
     ]
    }
   ],
   "source": [
    "############ DATA IMPORT\n",
    "\n",
    "# id data\n",
    "train = pd.read_csv('../raw/training.csv')\n",
    "test  = pd.read_csv('../raw/unlabeled.csv')\n",
    "\n",
    "\n",
    "# check dimensions\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train = train[-train['hospital_death'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 37.78 Mb (71.1% reduction)\n",
      "Mem. usage decreased to 16.01 Mb (71.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test  = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1LtthC77b22"
   },
   "outputs": [],
   "source": [
    "train['NAs'] = train.isnull().sum(axis=1)\n",
    "test['NAs']  = test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3CRPC3ms1Dj"
   },
   "outputs": [],
   "source": [
    "y     = train['hospital_death']\n",
    "train = train.drop('hospital_death', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['apache_4a_hospital_death_prob'][train['apache_4a_hospital_death_prob']==-1] = np.nan\n",
    "test['apache_4a_hospital_death_prob'][test['apache_4a_hospital_death_prob']==-1]   = np.nan\n",
    "\n",
    "train['apache_4a_icu_death_prob'][train['apache_4a_icu_death_prob']==-1] = np.nan\n",
    "test['apache_4a_icu_death_prob'][test['apache_4a_icu_death_prob']==-1]   = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test  = test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['pre_icu_los_days']<0]['pre_icu_los_days'] = 0\n",
    "test[test['pre_icu_los_days']<0]['pre_icu_los_days'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C42jQtu6_3gn"
   },
   "outputs": [],
   "source": [
    "train['apache_prob_prod'] = train['apache_4a_hospital_death_prob'] * train[ 'apache_4a_icu_death_prob']\n",
    "test['apache_prob_prod'] = test['apache_4a_hospital_death_prob'] * train[ 'apache_4a_icu_death_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJkR9Fc3b5Nb"
   },
   "outputs": [],
   "source": [
    "excluded_feats = ['encounter_id', 'patient_id', 'readmission_status', 'hospital_id', 'icu_id', 'icu_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hospital_id'] = train['hospital_id'].astype('object')\n",
    "test['hospital_id']  = test['hospital_id'].astype('object')\n",
    "\n",
    "train['icu_id'] = train['icu_id'].astype('object')\n",
    "test['icu_id']  = test['icu_id'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_factor'] = 0\n",
    "train.loc[train['age']<10, 'age_factor'] = 'under_10'\n",
    "train.loc[((train['age']>10) & (train['age']<20)), 'age_factor'] = 'b_10_20'\n",
    "train.loc[((train['age']>20) & (train['age']<35)), 'age_factor'] = 'b_20_35'\n",
    "train.loc[((train['age']>35) & (train['age']<50)), 'age_factor'] = 'b_35_50'\n",
    "train.loc[((train['age']>50) & (train['age']<70)), 'age_factor'] = 'b_50_70'\n",
    "train.loc[train['age']>70, 'age_factor'] = 'above_70'\n",
    "\n",
    "test['age_factor'] = 0\n",
    "test.loc[test['age']<10, 'age_factor'] = 'under_10'\n",
    "test.loc[((test['age']>10) & (train['age']<20)), 'age_factor'] = 'b_10_20'\n",
    "test.loc[((test['age']>20) & (train['age']<35)), 'age_factor'] = 'b_20_35'\n",
    "test.loc[((test['age']>35) & (train['age']<50)), 'age_factor'] = 'b_35_50'\n",
    "test.loc[((test['age']>50) & (train['age']<70)), 'age_factor'] = 'b_50_70'\n",
    "test.loc[test['age']>70, 'age_factor'] = 'above_70'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbp_risk1(train):\n",
    "\n",
    "    \"\"\"Categorises whether blood pressure is elevated, \n",
    " stage 1 hypertension or stage 2 hypertension\"\"\"\n",
    "    if train['d1_mbp_noninvasive_max'] >= 120 and train['d1_mbp_noninvasive_max'] < 130 and train['d1_mbp_noninvasive_min'] < 80:\n",
    "        bprisk = 'elevated BP'\n",
    "    elif (train['d1_mbp_noninvasive_max'] >= 130 and train['d1_mbp_noninvasive_max'] < 140) or (train['d1_mbp_noninvasive_min'] >= 80 and train['d1_mbp_noninvasive_min']< 90):\n",
    "        bprisk = 'stage 1 hypertension'\n",
    "    elif train['d1_mbp_noninvasive_max'] >= 140 or train['d1_mbp_noninvasive_min'] >= 90:\n",
    "         bprisk = 'stage 2 hypertension'\n",
    "    else:\n",
    "        bprisk = 'NAN'\n",
    "    return bprisk\n",
    "\n",
    "def mbp_risk2(train):\n",
    "\n",
    "    \"\"\"Categorises whether blood pressure is elevated, \n",
    " stage 1 hypertension or stage 2 hypertension\"\"\"\n",
    "    if train['d1_mbp_max'] >= 120 and train['d1_mbp_max'] < 130 and train['d1_mbp_min'] < 80:\n",
    "        bprisk = 'elevated BP'\n",
    "    elif (train['d1_mbp_max'] >= 130 and train['d1_mbp_max'] < 140) or (train['d1_mbp_min'] >= 80 and train['d1_mbp_min']< 90):\n",
    "        bprisk = 'stage 1 hypertension'\n",
    "    elif train['d1_mbp_max'] >= 140 or train['d1_mbp_min'] >= 90:\n",
    "         bprisk = 'stage 2 hypertension'\n",
    "    else:\n",
    "        bprisk = 'NAN'\n",
    "    return bprisk\n",
    "\n",
    "def mbp_risk3(train):\n",
    "\n",
    "    \"\"\"Categorises whether blood pressure is elevated, \n",
    " stage 1 hypertension or stage 2 hypertension\"\"\"\n",
    "    if train['d1_mbp_invasive_max'] >= 120 and train['d1_mbp_invasive_max'] < 130 and train['d1_mbp_invasive_min'] < 80:\n",
    "        bprisk = 'elevated BP'\n",
    "    elif (train['d1_mbp_invasive_max'] >= 130 and train['d1_mbp_invasive_max'] < 140) or (train['d1_mbp_invasive_min'] >= 80 and train['d1_mbp_invasive_min']< 90):\n",
    "        bprisk = 'stage 1 hypertension'\n",
    "    elif train['d1_mbp_invasive_max'] >= 140 or train['d1_mbp_invasive_min'] >= 90:\n",
    "         bprisk = 'stage 2 hypertension'\n",
    "    else:\n",
    "        bprisk = 'NAN'\n",
    "    return bprisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = 12\n",
    "Y1 =16\n",
    "\n",
    "def resprate_min_resp_classt(num):\n",
    "    if pd.isna(num):\n",
    "        return np.nan\n",
    "    if X1 <= num <= Y1:\n",
    "        return 'normal'\n",
    "    elif num <= 12:\n",
    "        return 'below normal'\n",
    "    elif num >= 16: \n",
    "        return 'above normal'\n",
    "    \n",
    "X2= 12\n",
    "Y2 =16\n",
    "\n",
    "def resprate_max_classt(num):\n",
    "    if pd.isna(num):\n",
    "        return np.nan\n",
    "    if X2 <= num <= Y2:\n",
    "        return 'normal'\n",
    "    elif num <= 12:\n",
    "        return 'below normal'\n",
    "    elif num >= 16: \n",
    "        return 'above normal'\n",
    "    \n",
    "X = 36.5\n",
    "Y =37.5\n",
    "\n",
    "def temp_classt(num):\n",
    "    if pd.isna(num):\n",
    "        return np.nan\n",
    "    if X <= num <= Y:\n",
    "        return 'normal'\n",
    "    elif num <= 36.4:\n",
    "        return 'below normal'\n",
    "    elif num >= 37.6: \n",
    "        return 'above normal'\n",
    "    \n",
    "def weighted_classt(x): \n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    elif x < 15: \n",
    "        return 'very severely underweight' \n",
    "    elif x >= 15 and x < 16: \n",
    "        return 'severely weight' \n",
    "    elif x >=16 and x < 18.5: \n",
    "        return 'underweight' \n",
    "    elif x >= 18.5 and x < 25: \n",
    "        return 'healthy weight' \n",
    "    elif x >= 25 and x < 30: \n",
    "        return 'overweight'\n",
    "    elif x >= 30 and x < 35: \n",
    "        return 'class 1' \n",
    "    elif x >= 35 and x < 40: \n",
    "        return 'class 2' \n",
    "    else: \n",
    "        return 'class 3' \n",
    " \n",
    "    \n",
    "    \n",
    "train['resprate_min'] = train['d1_resprate_min'].map(resprate_min_resp_classt)\n",
    "test['resprate_min'] = test['d1_resprate_min'].map(resprate_min_resp_classt)\n",
    "train['resprate_max'] = train['d1_resprate_max'].map(resprate_max_classt)\n",
    "test['resprate_max'] = test['d1_resprate_max'].map(resprate_max_classt)\n",
    "train['temp_class'] = train['temp_apache'].map(temp_classt)\n",
    "test['temp_class'] = test['temp_apache'].map(temp_classt)\n",
    "train['weightclass'] = train['bmi'].map(weighted_classt)\n",
    "test['weightclass'] = test['bmi'].map(weighted_classt)\n",
    "train['mean_reperate'] = train['d1_resprate_min'] + (train['d1_resprate_min']/2)\n",
    "test['mean_reperate'] = test['d1_resprate_min']+ (test['d1_resprate_min']/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sodium_apache_classt(x): \n",
    "    if pd.isna(x):\n",
    "        return np.nan \n",
    "    elif x < 120:\n",
    "        return \"Very low\"\n",
    "    elif x >= 120 and x < 134: \n",
    "        return 'Low Abnormal Range' \n",
    "    elif x >=135 and x < 154: \n",
    "        return 'Normal Range' \n",
    "    elif x >= 155: \n",
    "        return 'High Abnoral Range' \n",
    "train['sodium_apache_classt'] = train['sodium_apache'].apply(sodium_apache_classt)\n",
    "test['sodium_apache_classt'] = test['sodium_apache'].apply(sodium_apache_classt)\n",
    "\n",
    "def albumin_apache_classt(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    elif x < 20 :\n",
    "        return \"Very low\"\n",
    "    elif x >= 20 and x <= 24: \n",
    "        return  \"low\"\n",
    "    elif x >=25 and x <= 44: \n",
    "        return \"Normal\"\n",
    "    elif x >= 45: \n",
    "        return \"High\" \n",
    "train['albumin_apache_classt'] = train['albumin_apache'].apply(albumin_apache_classt)\n",
    "test['albumin_apache_classt'] = test['albumin_apache'].apply(albumin_apache_classt)\n",
    "def potassium_classt(x): \n",
    "    if pd.isna(x):\n",
    "        return np.nan \n",
    "    elif x < 2.5:\n",
    "        return \"Very Low\"\n",
    "    elif x >= 2.5 and x <= 3.4: \n",
    "        return 'Low Abnormal Range' \n",
    "    elif x >=3.5 and x <= 5.4: \n",
    "        return 'Normal Range' \n",
    "    elif x >= 5.5 and x <=7 : \n",
    "        return 'High Abnoral Range' \n",
    "    else: \n",
    "        return 'very High'\n",
    "train['potassium_min_classt'] = train['d1_potassium_min'].apply(potassium_classt)\n",
    "test['potassium_min_classt'] = test['d1_potassium_min'].apply(potassium_classt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['d1_diasbp_invasive', 'd1_diasbp', 'd1_diasbp_noninvasive', 'd1_heartrate','d1_mbp_invasive',\n",
    "              'd1_mbp', 'd1_mbp_noninvasive', 'd1_resprate', 'd1_spo2', 'd1_sysbp_invasive', 'd1_sysbp',\n",
    "              'd1_sysbp_noninvasive', 'd1_temp', 'h1_diasbp_invasive', 'h1_diasbp', 'h1_diasbp_noninvasive',\n",
    "              'h1_heartrate', 'h1_mbp_invasive', 'h1_mbp', 'h1_mbp_noninvasive', 'h1_resprate', 'h1_spo2',\n",
    "              'h1_sysbp_invasive', 'h1_sysbp', 'h1_sysbp_noninvasive', 'h1_temp', 'd1_albumin',\n",
    "              'd1_bilirubin', 'd1_bun', 'd1_calcium', 'd1_creatinine', 'd1_glucose', 'd1_hco3',\n",
    "              'd1_hemaglobin', 'd1_hematocrit', 'd1_inr', 'd1_lactate', 'd1_platelets', 'd1_potassium',\n",
    "              'd1_sodium', 'd1_wbc', 'h1_albumin', 'h1_bilirubin', 'h1_bun', 'h1_calcium',\n",
    "              'h1_creatinine', 'h1_glucose', 'h1_hco3', 'h1_hemaglobin', 'h1_hematocrit', 'h1_inr',\n",
    "              'h1_lactate', 'h1_platelets', 'h1_potassium', 'h1_sodium', 'h1_wbc', 'd1_arterial_pco2',\n",
    "              'd1_arterial_ph', 'd1_arterial_po2', 'd1_pao2fio2ratio', 'h1_arterial_pco2',\n",
    "              'h1_arterial_ph', 'h1_pao2fio2ratio']:\n",
    "    \n",
    "    train[f'{column}_span'] = train[f'{column}_max'] - train[f'{column}_min']\n",
    "    test[f'{column}_span'] = test[f'{column}_max'] - test[f'{column}_min']               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(train.isnull().sum(axis=0))\n",
    "excluded_feats.extend(list(nulls[nulls[0]>70000].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in train.columns if f not in excluded_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ PARAMETERS\n",
    "\n",
    "# cores\n",
    "cores = 20\n",
    "# cross-validation\n",
    "num_folds = 10\n",
    "shuffle   = True\n",
    "\n",
    "seed = 111\n",
    "\n",
    "# number of trees\n",
    "max_rounds = 10000\n",
    "stopping   = 400\n",
    "verbose    = 250\n",
    "\n",
    "# LGB parameters\n",
    "lgb_params = {\n",
    "    'boosting_type':     'goss',\n",
    "    'objective':         'Logloss',\n",
    "    'eval_metric':            'auc',\n",
    "    #'bagging_fraction':  0.5,\n",
    "   # 'bagging_freq':      100,\n",
    "   # 'feature_fraction':  0.5,\n",
    "   # 'lambda_l1':         0.1,\n",
    "    'l2_leaf_reg':         0.5,\n",
    "   # 'min_split_gain':    0.1,\n",
    "   # 'min_child_weight':  0,\n",
    "   # 'min_child_samples': 10,\n",
    "   # 'min_data':          50,\n",
    "   # 'silent':            True,\n",
    "   # 'verbosity':         -1,\n",
    "    'learning_rate':     0.01,\n",
    "    'max_depth':         5,\n",
    "   # 'num_leaves':       64,\n",
    "   # 'scale_pos_weight':  1,\n",
    "    'iterations':        max_rounds,\n",
    "    'thread_count' :     cores,\n",
    "    'random_state':      seed,\n",
    "    #\"device\" : \"gpu\"\n",
    "}\n",
    "\n",
    "# data partitinoing\n",
    "folds = StratifiedKFold(n_splits = num_folds, random_state = seed, shuffle = shuffle)\n",
    "\n",
    "############ PLACEHOLDERS\n",
    "\n",
    "# placeholders\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "\n",
    "# predictions\n",
    "preds_test   = np.zeros(test.shape[0])\n",
    "preds_oof    = np.zeros(train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 430526,
     "status": "error",
     "timestamp": 1579794901397,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64",
      "userId": "01253993997636551956"
     },
     "user_tz": -60
    },
    "id": "q44bOYsSs1Ds",
    "outputId": "6e0f98e9-f2bd-4176-db38-b8ea5d0b2653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (82541, 12) (9172, 12)\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "Bad value for num_feature[non_default_doc_idx=0,feature_idx=0]=\"Caucasian\": Cannot convert 'b'Caucasian'' to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert 'b'Caucasian'' to float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-82a5b2d490e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m#  eval_metric           = 'auc',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                   \u001b[0mearly_stopping_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                   verbose               = verbose)\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mclfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wids/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   3842\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   3843\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3844\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   3845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wids/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0msave_snapshot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wids/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         train_pool = _build_train_pool(X, y, cat_features, text_features, pairs, sample_weight, group_id,\n\u001b[0;32m-> 1602\u001b[0;31m                                        group_weight, subgroup_id, pairs_weight, baseline, column_description)\n\u001b[0m\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wids/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_build_train_pool\u001b[0;34m(X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n\u001b[0;32m-> 1003\u001b[0;31m                           group_weight=group_weight, subgroup_id=subgroup_id, pairs_weight=pairs_weight, baseline=baseline)\n\u001b[0m\u001b[1;32m   1004\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wids/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, cat_features, text_features, column_description, pairs, delimiter, has_header, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[1;32m    382\u001b[0m                     )\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/wids/lib/python3.6/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, data, label, cat_features, text_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_baseline_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.create_num_factor_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=0]=\"Caucasian\": Cannot convert 'b'Caucasian'' to float"
     ]
    }
   ],
   "source": [
    "############ CROSS-VALIDATION LOOP\n",
    "cv_start  = time.time()\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, y)):\n",
    "\n",
    "    # data partitioning\n",
    "    trn_x, trn_y = train[features].iloc[trn_idx], y.iloc[trn_idx]\n",
    "    val_x, val_y = train[features].iloc[val_idx], y.iloc[val_idx]\n",
    "    test_x       = test[features]\n",
    "    \n",
    "         \n",
    "    # Fill Na\n",
    "    for feature in ['ethnicity', 'gender']:\n",
    "      trn_x[feature]  = trn_x[feature].fillna(trn_x[feature].mode()[0])\n",
    "      val_x[feature]  = val_x[feature].fillna(trn_x[feature].mode()[0])\n",
    "      test_x[feature] = test_x[feature].fillna(trn_x[feature].mode()[0])\n",
    "\n",
    "      \n",
    "    for feature in ['hospital_admit_source', 'icu_admit_source', 'icu_stay_type']:#, 'icu_type']:\n",
    "      trn_x[feature]  = trn_x[feature].fillna('missing')\n",
    "      val_x[feature]  = val_x[feature].fillna('missing')\n",
    "      test_x[feature] = test_x[feature].fillna('missing')\n",
    "    \n",
    "    trn_x = pd.concat([trn_x, trn_y], axis=1)\n",
    "    val_x = pd.concat([val_x, val_y], axis=1)\n",
    "    \n",
    "    '''for feature in trn_x.select_dtypes('object').columns:    \n",
    "        targetc = KFoldTargetEncoderTrain(feature,'hospital_death',n_fold=10)\n",
    "        trn_x = targetc.fit_transform(trn_x)\n",
    "\n",
    "        test_targetc = KFoldTargetEncoderTest(trn_x,\n",
    "                                              feature,\n",
    "                                              f'{feature}_Kfold_Target_Enc')\n",
    "        test_x = test_targetc.fit_transform(test_x)\n",
    "        \n",
    "        val_targetc = KFoldTargetEncoderTest(trn_x,\n",
    "                                              feature,\n",
    "                                              f'{feature}_Kfold_Target_Enc')\n",
    "        val_x = val_targetc.fit_transform(val_x)'''\n",
    "    \n",
    "        \n",
    "    \n",
    "    trn_y = trn_x['hospital_death']\n",
    "    trn_x = trn_x.drop('hospital_death', axis=1)\n",
    "    \n",
    "    val_y = val_x['hospital_death']\n",
    "    val_x = val_x.drop('hospital_death', axis=1)\n",
    "    \n",
    "\n",
    "    for column in trn_x.select_dtypes('object').columns:\n",
    "        trn_x[column] = trn_x[column].fillna('')\n",
    "        val_x[column] = val_x[column].fillna('')\n",
    "        test_x[column] = test_x[column].fillna('')\n",
    "        \n",
    "\n",
    "    for column in ['urineoutput_apache','lymphoma', 'd1_potassium_min',\n",
    "                  'd1_heartrate_min', 'd1_mbp_noninvasive_max', \n",
    "                  'd1_creatinine_min' , 'd1_platelets_min',]:\n",
    "        trn_x[f'{column}_given'] = trn_x[column].isna()\n",
    "        val_x[f'{column}_given'] = val_x[column].isna()\n",
    "        test_x[f'{column}_given'] = test_x[column].isna()\n",
    "    \n",
    "    \n",
    "    # label encoding\n",
    "    #trn_x, val_x, test_x = label_encoding(trn_x, val_x, test_x)\n",
    "       \n",
    "    ## add noise to train to reduce overfitting\n",
    "    #trn_x += np.random.normal(0, 0.1, trn_x.shape)\n",
    "    \n",
    "    trn_x = trn_x.select_dtypes('object')\n",
    "    val_x = val_x.select_dtypes('object')\n",
    "    test_x = test_x.select_dtypes('object')\n",
    "    \n",
    "    # print data dimensions\n",
    "    print('Data shape:', trn_x.shape, val_x.shape)\n",
    "    #print('Data shape:', trn_y.shape, val_y.shape)    \n",
    "    # train lightGBM\n",
    "    clf = CatBoostClassifier(**lgb_params) \n",
    "    clf = clf.fit(trn_x, trn_y, \n",
    "                  eval_set              = [(trn_x, trn_y), (val_x, val_y)], \n",
    "                #  eval_metric           = 'auc', \n",
    "                  early_stopping_rounds = stopping,\n",
    "                  verbose               = verbose)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "    # find the best iteration\n",
    "    best_iter = clf.best_iteration_\n",
    "\n",
    "    # save predictions\n",
    "    preds_oof[val_idx] = clf.predict_proba(val_x,  num_iteration = best_iter)[:, 1]\n",
    "    preds_test        += clf.predict_proba(test_x, num_iteration = best_iter)[:, 1] / folds.n_splits \n",
    "\n",
    "    # importance\n",
    "    fold_importance_df               = pd.DataFrame()\n",
    "    fold_importance_df['Feature']    = trn_x.columns\n",
    "    fold_importance_df['Importance'] = clf.feature_importances_\n",
    "    fold_importance_df['Fold']       = n_fold + 1\n",
    "    importances                      = pd.concat([importances, fold_importance_df], axis = 0)\n",
    "    \n",
    "    # print performance\n",
    "    print('--------------------------------')\n",
    "    print('FOLD%2d: AUC = %.6f' % (n_fold + 1, roc_auc_score(y[val_idx], preds_oof[val_idx])))\n",
    "    print('--------------------------------')\n",
    "    print('')\n",
    "        \n",
    "    # clear memory\n",
    "    del trn_x, trn_y, val_x, val_y\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "# print overall performance    \n",
    "cv_perf = roc_auc_score(y, preds_oof)\n",
    "print('--------------------------------')\n",
    "print('- OOF AUC = %.6f' % cv_perf)\n",
    "print('- CV TIME = {:.2f} min'.format((time.time() - cv_start) / 60))\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWHIEpSds1Du"
   },
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZNWGMc6s1Dv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90729\n"
     ]
    }
   ],
   "source": [
    "############ RECHECK PERFORMANCE  \n",
    "\n",
    "\n",
    "# check performance\n",
    "print(np.round(roc_auc_score(y, preds_oof), 5))\n",
    "\n",
    "\n",
    "############ TRACK RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2w6VAeHs1Dw"
   },
   "outputs": [],
   "source": [
    "############ VARIABLE IMPORTANCE\n",
    "\n",
    "# load importance    \n",
    "top_feats = 300\n",
    "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
    "importance = importances.loc[importances.Feature.isin(cols)]\n",
    "    \n",
    "# plot variable importance\n",
    "plt.figure(figsize = (10, 150))\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False))\n",
    "plt.tight_layout()\n",
    "plt.savefig('./var_importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UbGUmx1s1Dy"
   },
   "source": [
    "SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1579793483507,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64",
      "userId": "01253993997636551956"
     },
     "user_tz": -60
    },
    "id": "Ut1uW95ds1Dy",
    "outputId": "e9f224fc-7769-45d9-85a5-92ccead9e441"
   },
   "outputs": [],
   "source": [
    "# file name\n",
    "model = 'goss_v17_seed111'\n",
    "perf  = str(round(cv_perf, 6))[2:7]\n",
    "name  = model + '_' + perf\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1579793484594,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64",
      "userId": "01253993997636551956"
     },
     "user_tz": -60
    },
    "id": "ox2RkuQks1D0",
    "outputId": "1c58a4f9-c838-44c5-da4a-01705b9c9b64"
   },
   "outputs": [],
   "source": [
    "# export OOF preds\n",
    "oof = pd.DataFrame({'encounter_id': train['encounter_id'], 'hospital_death': preds_oof})\n",
    "oof.to_csv('../oof_preds/' + str(name) + '.csv', index = False)\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(preds_test.reshape(-1,1))\n",
    "preds_test = scaler.transform(preds_test.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1579793485711,
     "user": {
      "displayName": "Elizaveta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64",
      "userId": "01253993997636551956"
     },
     "user_tz": -60
    },
    "id": "JprRz1pPs1D1",
    "outputId": "b5516a30-8799-4813-94af-c40870ea0e94"
   },
   "outputs": [],
   "source": [
    "# export submission\n",
    "sub = pd.DataFrame({'encounter_id': test['encounter_id'], 'hospital_death': preds_test})\n",
    "sub.to_csv('../submissions/' + str(name) + '.csv', index = False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "code_1_lgb_main_COLAB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wids",
   "language": "python",
   "name": "wids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
